{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COURSE PROJECT: NEURAL NETWORKS FOR CLASSIFICATION\n",
    "\n",
    "**SINH VIÊN 1**\n",
    "\n",
    "Họ tên: Nguyễn Hoàng Khải Minh\n",
    "\n",
    "MSSV: 22127267\n",
    "\n",
    "<br>\n",
    "\n",
    "**SINH VIÊN 2**\n",
    "\n",
    "Họ tên: Lê Thị Ngọc Linh\n",
    "\n",
    "MSSV: 22127232\n",
    "\n",
    "<br>\n",
    "(Cập nhật lần cuối: 23/11/2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import các thư viện cần thiết"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bộ dữ liệu được sử dụng trong đồ án này chính là CIFAR-10, gồm 60,000 hình ảnh 32x32 pixel, được chia thành 10 lớp:\n",
    "<ul>\n",
    "<li> Máy bay </li>\n",
    "<li> Xe máy </li>\n",
    "<li> Chim </li>\n",
    "<li> Mèo </li>\n",
    "<li> Hươu </li>\n",
    "<li> Chó </li>\n",
    "<li> Ếch </li>\n",
    "<li> Ngựa </li>\n",
    "<li> Tàu </li>\n",
    "<li> Xe tải </li>\n",
    "</ul>\n",
    "\n",
    "Tập dữ liệu gồm 60,000 ảnh, được chia thành 50,000 ảnh cho tập huấn luyện và 10,000 ảnh dùng cho tập kiểm tra.\n",
    "\n",
    "Hàm read_cifar() bên dưới đọc dữ liệu từ 5 batch trong tập dữ liệu train và lưu vào train_X, train_Y, sau đó đọc dữ liệu từ test batch và lưu vào test_X, test_Y, tổng cộng trả về 4 mảng train_X, train_Y, test_X, test_Y. Trong hàm đồng thời chuẩn hóa dữ liệu từ 0 - 255 (giá trị của 1 chanel tronng pixel) về đoạn 0 - 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar():\n",
    "    train_X = []\n",
    "    train_Y = []\n",
    "\n",
    "    for i in range (1, 6):\n",
    "        with open(f'cifar-10-batches-py/data_batch_{i}', 'rb') as f:\n",
    "            train_data = pickle.load(f, encoding='bytes')\n",
    "            train_X.append((train_data[b'data']) / 255.0)\n",
    "            train_Y.append(train_data[b'labels'])\n",
    "    train_X = np.concatenate(train_X, axis=0)\n",
    "    train_Y = np.concatenate(train_Y)\n",
    "    \n",
    "    with open('cifar-10-batches-py/test_batch', 'rb') as f:\n",
    "        test_data = pickle.load(f, encoding='bytes')\n",
    "        test_X = np.array(test_data[b'data']) / 255.0\n",
    "        test_Y = np.array(test_data[b'labels'])\n",
    "\n",
    "    return train_X, train_Y, test_X, test_Y\n",
    "\n",
    "\n",
    "train_X, train_Y, test_X, test_Y = read_cifar()\n",
    "\n",
    "print(f'Shape of train_X: {train_X.shape}, shape of train_y: {train_Y.shape}')\n",
    "print(f'Shape of test_X:  {test_X.shape}, shape of test_y:  {test_Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Min of train_X: {train_X.min()}, max of train_X: {train_X.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi đọc sau, có thể xem qua một số ảnh trong bộ dữ liệu CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rimages = 8; n_cimages = 8 \n",
    "padding = 2 \n",
    "canvas = 0.5 * np.ones((n_rimages * (32 + 2 * padding), n_cimages * (32 + 2 * padding), 3))\n",
    "rand_idxs = np.random.permutation(np.arange(len(train_X))[:n_rimages * n_cimages])\n",
    "for r in range(n_rimages):\n",
    "    for c in range(n_cimages):\n",
    "        i = r * n_cimages + c\n",
    "        image = train_X[rand_idxs[i]].reshape(3, 32, 32).transpose(1, 2, 0)\n",
    "        temp1 = r * (32 + 2 * padding) + padding \n",
    "        temp2 = c * (32 + 2 * padding) + padding \n",
    "        canvas[temp1:temp1 + 32, temp2:temp2 + 32, :] = image\n",
    "plt.imshow(canvas, vmin=0, vmax=1)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tiếp theo, in ra các nhãn có trong tập huấn luyện và số lượng của mỗi nhãn. Có thể thấy cả 10 nhãn trong tập huấn luyện đều có số lượng bằng nhau là 5,000 hình cho mỗi nhãn, có thể thấy số lượng mỗi nhãn đều nhau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(meta_file):\n",
    "    with open(meta_file, 'rb') as f:\n",
    "        meta_data = pickle.load(f, encoding='bytes')\n",
    "    return meta_data[b'label_names']\n",
    "\n",
    "# Ví dụ:\n",
    "meta_file = 'cifar-10-batches-py/batches.meta'\n",
    "label_names = load_meta(meta_file)\n",
    "\n",
    "values, counts = np.unique(train_Y, return_counts=True)\n",
    "for value, count in zip(values, counts):\n",
    "    print(f'Value: {value} - {label_names[value].decode('utf-8')}, count: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng chính là hàm add_ones(), truyền vào ma trận X sau đó thêm cột 1 vào làm bias để hoàn thiện việc toàn xử lý. Ngoài ra có thêm hàm one_hot để encoding mảng output Y từ 0-9 về dạng one hot, để thuận tiện cho việc sử dụng các thư viện huấn luyện yêu cầu đầu vào của mảng Y phải là one hot như thư viện tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones(X):\n",
    "    return np.hstack((np.ones((len(X), 1)), X))\n",
    "\n",
    "def one_hot(Y):\n",
    "    K = np.max(Y) + 1\n",
    "    y_onehot = np.zeros((len(Y), K))\n",
    "    y_onehot[np.arange(len(Y)), Y] = 1\n",
    "    return y_onehot\n",
    "\n",
    "# Gọi hàm add_ones để tiền xử lý train_X\n",
    "train_Z = add_ones(train_X)\n",
    "print(f'Shape of train_Z: {train_Z.shape}')\n",
    "\n",
    "# Gọi hàm one_hot để tiền xử lý train_Y\n",
    "train_Y_onehot = one_hot(train_Y)\n",
    "train_Y_onehot.shape\n",
    "print(f'Shape of train_Y_onehot: {train_Y_onehot.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Hyperparameter        | Value             |\n",
    "|-----------------------|-------------------|\n",
    "| Number of layers      | 2 hidden layers   |\n",
    "| Neurons each layer    | 512, 512          |\n",
    "| Activation function   | ReLU              |\n",
    "| Optimizer             | SGD               |\n",
    "| Weight decay          | 0.001             |\n",
    "| Momentum              | 0                 |\n",
    "| Learning rate init    | 0.1               |\n",
    "| Learning rate decay   | 0.1               |\n",
    "| Decay step            | 30 epochs         |\n",
    "| Epochs                | 100               |\n",
    "| Batch size            | 128               |\n",
    "| Loss function         | Cross-entropy     |\n",
    "| Validation fraction   | 0.1               |\n",
    "| Early stopping        | True              |\n",
    "| Stop from epoch       | 90                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SKLEARN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_sklearn(X, y, verbose=False):\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    # Số epochs cho mỗi chu kỳ\n",
    "    epochs_per_cycle = 30\n",
    "    total_epochs = 100\n",
    "    initial_lr = 0.1\n",
    "    \n",
    "    # Tính số chu kỳ\n",
    "    num_cycles = total_epochs // epochs_per_cycle\n",
    "\n",
    "    # Mô hình MLPClassifier\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(512, 512),\n",
    "        activation='relu',\n",
    "        alpha=0.001,\n",
    "        solver='sgd',\n",
    "        momentum=0,\n",
    "        learning_rate_init=initial_lr,\n",
    "        batch_size=128,\n",
    "        validation_fraction=0.1,\n",
    "        max_iter=epochs_per_cycle,\n",
    "        n_iter_no_change=epochs_per_cycle,\n",
    "        early_stopping=True,\n",
    "        verbose=verbose,\n",
    "        warm_start=True  # Giữ lại trạng thái mô hình khi huấn luyện thêm\n",
    "    )\n",
    "\n",
    "    # Huấn luyện mô hình với giảm learning rate sau mỗi chu kỳ\n",
    "    for cycle in range(num_cycles):\n",
    "        if (verbose):\n",
    "            print(f\"\\n=== Cycle {cycle+1}/{num_cycles} ===\")\n",
    "        model.learning_rate_init = initial_lr * (10 ** -cycle)  # Giảm lr xuống 10 lần mỗi chu kỳ\n",
    "        if (verbose):\n",
    "            print(f\"Learning rate: {model.learning_rate_init}\")\n",
    "        if cycle == num_cycles - 1:\n",
    "            model.n_iter_no_change = 1\n",
    "        model.fit(X, y)\n",
    "    return model, {'train_loss': model.loss_curve_, 'val_loss': model.validation_scores_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình\n",
    "model_sklearn, histor_sklearn = mlp_sklearn(train_Z, train_Y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán\n",
    "test_Z = add_ones(test_X)\n",
    "test_Y_pred = model_sklearn.predict(test_Z)\n",
    "\n",
    "# Tính độ chính xác\n",
    "accuracy = accuracy_score(test_Y, test_Y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# In classification report\n",
    "print(classification_report(test_Y, test_Y_pred, target_names=[label.decode('utf-8') for label in label_names]))\n",
    "\n",
    "# In confusion matrix\n",
    "cm = confusion_matrix(test_Y, test_Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_sklearn.loss_curve_, label='Training loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(model_sklearn.validation_scores_, label='Validation scores', color='b')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation scores')\n",
    "plt.title('Validation scores')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TENSORFLOW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers: 2 hidden layers\n",
    "# Neuron per layer: 512\n",
    "# Activation function: ReLU\n",
    "# Optimizer: Adam\n",
    "# Learning rate: 0.001 (default)\n",
    "# Weight decay: 0.0001 \n",
    "# epsilon: 1e-7 (default)\n",
    "# Epochs: 100\n",
    "# Batch size: 200\n",
    "# Loss function: Cross-entropy\n",
    "\n",
    "def mlp_tf(X, y, verbose=False):\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras import layers, models\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(X.shape[1], )),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.1,\n",
    "        decay_steps=30*(X.shape[0]*0.9//128),\n",
    "        decay_rate=0.1,\n",
    "        staircase=True\n",
    "    )\n",
    "\n",
    "    sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, weight_decay=0.001)\n",
    "\n",
    "    model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(\n",
    "        X, \n",
    "        one_hot(y), \n",
    "        epochs=100, \n",
    "        batch_size=128, \n",
    "        validation_split=0.1,\n",
    "        callbacks=[tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            patience=0,\n",
    "            min_delta=0,\n",
    "            restore_best_weights=True,\n",
    "            start_from_epoch=60\n",
    "        )],\n",
    "        verbose=verbose\n",
    "    )\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình\n",
    "model_tf, history_tf = mlp_tf(train_Z, train_Y, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán\n",
    "test_Z = add_ones(test_X)\n",
    "test_Y_pred = model_tf.predict(test_Z)\n",
    "test_Y_pred = np.argmax(test_Y_pred, axis=1)\n",
    "\n",
    "# Tính độ chính xác\n",
    "accuracy = accuracy_score(test_Y, test_Y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# In classification report\n",
    "print(classification_report(test_Y, test_Y_pred, target_names=[label.decode('utf-8') for label in label_names]))\n",
    "\n",
    "# In confusion matrix\n",
    "cm = confusion_matrix(test_Y, test_Y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_tf.history['accuracy'], label='Train')\n",
    "plt.plot(history_tf.history['val_accuracy'], label='Validation')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_tf.history['loss'], label='Train')\n",
    "plt.plot(history_tf.history['val_loss'], label='Validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PYTORCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, ouput_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, ouput_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "def mlp_pytorch(X, y, verbose=False):\n",
    "    epochs = 100\n",
    "    batch_size = 128\n",
    "    patience = 0\n",
    "    start_from_epoch = 60\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X = torch.tensor(X, dtype=torch.float32)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    \n",
    "    # Split data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model, loss, and optimizer\n",
    "    input_size = X.shape[1]\n",
    "    output_size = 10\n",
    "    model = MLP(input_size, output_size)\n",
    "\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, weight_decay=0.001)\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"val_accuracy\": []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item() * X_batch.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                correct += (preds == y_batch).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy = correct / len(val_loader.dataset)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_accuracy\"].append(val_accuracy)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch >= start_from_epoch:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        if (verbose):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, \"\n",
    "                f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, \"\n",
    "                f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Load the best model\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "# Dự đoán\n",
    "def pytorch_predict(model, test_X):\n",
    "    model.eval()  # Đặt mô hình ở chế độ đánh giá\n",
    "    with torch.no_grad():  # Tắt tính toán gradient\n",
    "        # Thêm cột 1s (nếu cần)\n",
    "        test_Z = torch.tensor(test_X, dtype=torch.float32)  # Convert test_X to PyTorch tensor\n",
    "        \n",
    "        # Dự đoán xác suất\n",
    "        outputs = model(test_Z)\n",
    "        \n",
    "        # Lấy nhãn dự đoán (argmax trên xác suất)\n",
    "        test_Y_pred = outputs.argmax(dim=1).numpy()  # Chuyển về numpy array\n",
    "    return test_Y_pred \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Huấn luyện mô hình\n",
    "model_pytorch, hystory_pytorch = mlp_pytorch(train_Z, train_Y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dự đoán\n",
    "test_Z = add_ones(test_X)\n",
    "test_Y_pred = pytorch_predict(model_pytorch, test_Z)\n",
    "\n",
    "# Tính độ chính xác\n",
    "accuracy = accuracy_score(test_Y, test_Y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# In classification report\n",
    "print(classification_report(test_Y, test_Y_pred, target_names=[label.decode('utf-8') for label in label_names]))\n",
    "\n",
    "# In confusion matrix\n",
    "cm = confusion_matrix(test_Y, test_Y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hystory_pytorch['train_loss'], label='Train')\n",
    "plt.plot(hystory_pytorch['val_loss'], label='Validation')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hystory_pytorch['val_accuracy'], label='Validation')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import psutil\n",
    "import tracemalloc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "train_function = {\n",
    "    'sklearn': mlp_sklearn,\n",
    "    'tf': mlp_tf,\n",
    "    'pytorch': mlp_pytorch,\n",
    "}\n",
    "\n",
    "model = []\n",
    "history = []\n",
    "time_measure = []\n",
    "memory_measure = []\n",
    "\n",
    "for key, value in train_function.items():\n",
    "    print(f\"Training with {key}\")\n",
    "    start = time.time()\n",
    "\n",
    "    # Đo memory usage\n",
    "    tracemalloc.start()\n",
    "\n",
    "    # Huấn luyện model\n",
    "    model_, history_ = value(train_Z, train_Y)\n",
    "\n",
    "    # Đo memory usage\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    # Đo thời gian và GPU usage\n",
    "    end = time.time()\n",
    "    model.append(model_)\n",
    "    history.append(history_)\n",
    "    time_measure.append((end - start) / 60)\n",
    "    memory_measure.append(current / 10**6)\n",
    "\n",
    "    print(f\"{key} - Time: {(end - start) / 60:.2f} min, Memory Usage: {memory_measure[-1]:.2f} MB\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_Z = add_ones(test_X)\n",
    "\n",
    "accuracy = []\n",
    "model_confusion_matrix = []\n",
    "model_classification_report = []\n",
    "\n",
    "model_predict = {\n",
    "    'sklearn': model[0].predict(test_Z),\n",
    "    'tf': model[1].predict(test_Z),\n",
    "    'pytorch': pytorch_predict(model[2], test_Z),\n",
    "}\n",
    "\n",
    "\n",
    "for key, predict_ in model_predict.items():\n",
    "    if key == 'tf':\n",
    "        predict_ = np.argmax(predict_, axis=1)\n",
    "    accuracy_ = accuracy_score(test_Y, predict_)\n",
    "    confusion_matrix_ = confusion_matrix(test_Y, predict_)\n",
    "    classification_report_ = classification_report(test_Y, predict_, target_names=[label.decode('utf-8') for label in label_names])\n",
    "\n",
    "    accuracy.append(accuracy_)\n",
    "    model_confusion_matrix.append(confusion_matrix_)\n",
    "    model_classification_report.append(classification_report_)\n",
    "\n",
    "    print(\"====================================\")\n",
    "    print(f\"Accuracy of {key}: {accuracy_ * 100:.2f}%\")\n",
    "    print(f\"Confusion matrix of {key}:\")\n",
    "    print(confusion_matrix_)\n",
    "    print(f\"Classification report of {key}:\")\n",
    "    print(classification_report_)\n",
    "    print()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time of each library\n",
    "plt.figure(figsize=(12, 4))\n",
    "bars = plt.bar(train_function.keys(), time_measure)\n",
    "plt.bar_label(bars)\n",
    "plt.ylabel('Time (minutes)')\n",
    "plt.title('Training time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot memory usage of each library\n",
    "plt.figure(figsize=(12, 4))\n",
    "bars = plt.bar(train_function.keys(), memory_measure)\n",
    "plt.bar_label(bars, fmt='%.0f', label_type='edge')  \n",
    "plt.ylabel('Memory (MB)')\n",
    "plt.title('Memory usage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "bars = plt.bar(train_function.keys(), accuracy)\n",
    "plt.bar_label(bars, fmt='%.2f', label_type='edge')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_percent = [a * 100 for a in accuracy]\n",
    "\n",
    "# Vẽ biểu đồ\n",
    "n = len(train_function)\n",
    "index = np.arange(n)\n",
    "width = 0.25  # Độ rộng của mỗi cột\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "bars1 = plt.bar(index - width, time_measure, width, label='Time (minutes)')\n",
    "bars2 = plt.bar(index, memory_measure, width, label='Memory (MB)')\n",
    "bars3 = plt.bar(index + width, accuracy_percent, width, label='Accuracy (%)')\n",
    "\n",
    "# Hiển thị giá trị trên cột\n",
    "plt.bar_label(bars1, fmt='%.0f')\n",
    "plt.bar_label(bars2, fmt='%.0f')\n",
    "plt.bar_label(bars3, fmt='%.2f')\n",
    "\n",
    "# Thiết lập trục và tiêu đề\n",
    "plt.xticks(index, train_function)\n",
    "plt.ylabel('Values')\n",
    "plt.title('Comparison of Time, Memory, and Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sklearn_loss = model[0].loss_curve_\n",
    "tf_loss = history[1].history['loss']\n",
    "pytorch_loss = history[2]['train_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sklearn_loss, label='Sklearn', color='r')\n",
    "plt.plot(tf_loss, label='Tensorflow', color='g')\n",
    "plt.plot(pytorch_loss, label='Pytorch', color='b')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_learn_val_loss = model[0].validation_scores_\n",
    "tf_val_loss = history[1].history['val_accuracy']\n",
    "pytorch_val_loss = history[2]['val_accuracy']\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(sk_learn_val_loss, label='Sklearn', color='r')\n",
    "plt.plot(tf_val_loss, label='Tensorflow', color='g')\n",
    "plt.plot(pytorch_val_loss, label='Pytorch', color='b')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, cm in zip(train_function.keys(), model_confusion_matrix):\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt='d',\n",
    "        cmap='Blues',\n",
    "        xticklabels=[name.decode('utf-8') for name in label_names],\n",
    "        yticklabels=[name.decode('utf-8') for name in label_names]\n",
    "    )\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix of {key}')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
